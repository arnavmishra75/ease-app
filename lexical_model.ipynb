{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/agentlans/text-quality/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fineweb</th>\n",
       "      <th>nvidia</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our razor sharp swords sliced through the roug...</td>\n",
       "      <td>allenai/c4</td>\n",
       "      <td>-0.438112</td>\n",
       "      <td>0.182096</td>\n",
       "      <td>-0.049947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In fact my own children attend several day cam...</td>\n",
       "      <td>allenai/c4</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.265714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The reduced inflammatory reaction causes a dec...</td>\n",
       "      <td>agentlans/wikipedia-paragraphs</td>\n",
       "      <td>0.905146</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>0.792037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The incredible spetaculo de la vida, the incre...</td>\n",
       "      <td>monology/pile-uncopyrighted</td>\n",
       "      <td>-0.566386</td>\n",
       "      <td>-0.072152</td>\n",
       "      <td>-0.299926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the upper slopes they clashed with a force ...</td>\n",
       "      <td>agentlans/wikipedia-paragraphs</td>\n",
       "      <td>1.135600</td>\n",
       "      <td>0.052069</td>\n",
       "      <td>0.734396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>Also included and released as a single is a so...</td>\n",
       "      <td>agentlans/wikipedia-paragraphs</td>\n",
       "      <td>-0.322785</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>-0.504086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>A:\\n\\nYou could simply move ....\\nResponse.Red...</td>\n",
       "      <td>monology/pile-uncopyrighted</td>\n",
       "      <td>0.640707</td>\n",
       "      <td>-1.240916</td>\n",
       "      <td>-0.975509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>The next year representatives met with counter...</td>\n",
       "      <td>agentlans/wikipedia-paragraphs</td>\n",
       "      <td>0.446387</td>\n",
       "      <td>0.170391</td>\n",
       "      <td>0.063359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>Anyway, I would love to hear your thoughts on ...</td>\n",
       "      <td>monology/pile-uncopyrighted</td>\n",
       "      <td>-1.005016</td>\n",
       "      <td>0.051131</td>\n",
       "      <td>-0.393147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>Representatives from organizations across the ...</td>\n",
       "      <td>HuggingFaceFW/fineweb-edu</td>\n",
       "      <td>-0.393774</td>\n",
       "      <td>0.491026</td>\n",
       "      <td>0.069158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      Our razor sharp swords sliced through the roug...   \n",
       "1      In fact my own children attend several day cam...   \n",
       "2      The reduced inflammatory reaction causes a dec...   \n",
       "3      The incredible spetaculo de la vida, the incre...   \n",
       "4      On the upper slopes they clashed with a force ...   \n",
       "...                                                  ...   \n",
       "89995  Also included and released as a single is a so...   \n",
       "89996  A:\\n\\nYou could simply move ....\\nResponse.Red...   \n",
       "89997  The next year representatives met with counter...   \n",
       "89998  Anyway, I would love to hear your thoughts on ...   \n",
       "89999  Representatives from organizations across the ...   \n",
       "\n",
       "                               source   fineweb    nvidia   quality  \n",
       "0                          allenai/c4 -0.438112  0.182096 -0.049947  \n",
       "1                          allenai/c4  0.150865  0.011652  0.265714  \n",
       "2      agentlans/wikipedia-paragraphs  0.905146  0.428094  0.792037  \n",
       "3         monology/pile-uncopyrighted -0.566386 -0.072152 -0.299926  \n",
       "4      agentlans/wikipedia-paragraphs  1.135600  0.052069  0.734396  \n",
       "...                               ...       ...       ...       ...  \n",
       "89995  agentlans/wikipedia-paragraphs -0.322785  0.029838 -0.504086  \n",
       "89996     monology/pile-uncopyrighted  0.640707 -1.240916 -0.975509  \n",
       "89997  agentlans/wikipedia-paragraphs  0.446387  0.170391  0.063359  \n",
       "89998     monology/pile-uncopyrighted -1.005016  0.051131 -0.393147  \n",
       "89999       HuggingFaceFW/fineweb-edu -0.393774  0.491026  0.069158  \n",
       "\n",
       "[90000 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"quality\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nulls(df):\n",
    "    return df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    return df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: [text, quality]\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: [text, quality]\n",
       " Index: [])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_duplicates(df), check_nulls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    90000.000000\n",
       "mean       136.165633\n",
       "std        367.651079\n",
       "min          1.000000\n",
       "25%         66.000000\n",
       "50%        109.000000\n",
       "75%        164.000000\n",
       "max      59098.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/nfqdx01s049dj1lj41zznlbm0000gn/T/ipykernel_49893/3778364382.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"embedding\"] = df[\"text\"].apply(lambda x: encoder.encode(x))\n"
     ]
    }
   ],
   "source": [
    "df[\"embedding\"] = df[\"text\"].apply(lambda x: encoder.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_embeddings(df):\n",
    "    embeddings_df = pd.DataFrame(df['embedding'].tolist())\n",
    "    embeddings_df.columns = [f'embedding_{i}' for i in range(embeddings_df.shape[1])]\n",
    "    df = df.drop(columns=['embedding']).join(embeddings_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = flatten_embeddings(df).drop(columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    90000.000000\n",
       "mean         0.000535\n",
       "std          0.681308\n",
       "min         -5.241319\n",
       "25%         -0.474049\n",
       "50%          0.010280\n",
       "75%          0.497374\n",
       "max          2.271465\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_df[\"quality\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df[\"quality\"] = standardize(numerical_df[\"quality\"]) # can train without scaling, if so use tanh activation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    numerical_df.drop(columns=[\"quality\"]),\n",
    "    numerical_df[\"quality\"],\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnavmishra/Documents/aavya synopsys competitions/aavya synopsys 2025/ease-app/evi-env/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(384,)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # use tanh activation if DIDN'T MINMAX SCALE target var\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0288 - val_loss: 0.0035\n",
      "Epoch 2/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 3/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 4/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 5/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 977us/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 6/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 979us/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 7/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 992us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 8/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 973us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 9/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 10/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 976us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 11/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 979us/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 12/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 976us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 13/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 978us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 14/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 980us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 15/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 16/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 986us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 17/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 18/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 968us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 19/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 968us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 20/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 980us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 21/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 998us/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 22/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 967us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 23/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 978us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 24/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 972us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 25/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 981us/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 26/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 983us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 27/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 985us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 28/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 991us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 29/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 976us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 30/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 981us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 31/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 988us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 32/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 987us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 33/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 34/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 980us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 35/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 987us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 36/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 989us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 37/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 986us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 38/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 39/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 990us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 40/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 41/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 992us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 42/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 43/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 995us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 44/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 45/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 986us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 46/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 989us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 47/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 997us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 48/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 49/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 50/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 994us/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 51/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 993us/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 52/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 996us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 53/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 54/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 990us/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 55/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 993us/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 56/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 57/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 989us/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 58/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 59/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 60/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 61/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 62/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 63/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 64/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 65/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 998us/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 66/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1000us/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 67/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 68/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 69/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 70/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 71/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 72/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 73/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 74/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 75/75\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0013 - val_loss: 0.0025\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=75, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2250/2250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308us/step\n",
      "Train RMSE: 0.030407155902174198\n"
     ]
    }
   ],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "rmse_train = root_mean_squared_error(y_train, X_pred)\n",
    "print(f\"Train RMSE: {rmse_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step\n",
      "Test RMSE: 0.05078736239052238\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "rmse_test = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test RMSE: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"models/lexical_model.keras\") <- if to save the non-scaled outputs model\n",
    "model.save(\"models/lexical_model_scaled_outputs.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
